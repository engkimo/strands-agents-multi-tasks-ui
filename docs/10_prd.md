# PRD（プロダクト要求仕様）

## 背景 / 課題
- 複数の AI/CLI ツールを使い分ける現場では、どれが“このタスクで最適”かの判断・比較が困難。
- 実行の過程（入出力/ログ/コスト/レイテンシ）がサイロ化し、チームでの再現・レビューが難しい。
- 既存の可視化/オーケストレーションはベンダー/環境にロックされがちで、ローカル運用が弱い。

## 目標（MVP 時点）
- 同一入力に対する複数ツールの並列実行・比較が可能。
- 実行状況（DAG/Swarm）の可視化、ノードの I/O/ログ/トレース表示。
- 実行履歴からワンクリック再実行・差分比較。
- ローカル SQLite 等でのメタデータ保存（オフライン対応）。

## 成功指標（初期）
- 週次アクティブユーザー、1 セッション当たり実行数、成功率、平均レイテンシ。
- 比較実行（2 つ以上ツール）の利用比率、再実行（リプレイ）の利用比率。

## 決定事項（現時点）
- 対象ユーザー（Primary）: 個人開発者。
- 初期対応ツール: Claude Code / Codex CLI / Gemini CLI / Spec Kit。
- UI 形態: ローカル Web。
- データ方針: ミニマム（完全ローカル必須ではない）。
- MVP の必須体験: 並列比較 + 差分ビュー。
- 競合意識: Dify（差別化対象）。
- セキュリティ追加要件: なし（MVP 範囲）。
- 成果物（デモ）: docs/bounce_ideas_off_of_someone_with_AI.md にまとめる。
- リソース: 1 名体制。
- 開発環境: Python / Node / Docker。

## ペルソナ
- エンジニア A: 複数の AI コード支援 CLI を比較・使い分けたい個人開発者。
- チームリード B: チーム標準の“推奨ツール/設定”を決めたい評価担当。
- プラットフォーム C: 会社のガバナンス/コスト最適化の観点で可視化したい。

## ユーザーストーリー（例）
- 開発者として、同じプロンプトを 3 つの CLI に投げ、結果と差分を UI で見比べたい。
- チームとして、特定ドメインの評価タスクで最も高品質なツール/設定を決めたい。
- 管理者として、各ツールのエラー原因や平均レイテンシをダッシュボードで確認したい。

## 必須機能要件（MVP）
1. ツール登録/管理: Python ラッパー+`@tool` で CLI を登録。入出力スキーマ/ヘルプ定義。
2. 並列実行: `invoke_async` による複数ツール同時実行、タイムアウト/キャンセル対応。
3. フロー構築: Graph/Swarm によるノード/エッジ定義、条件分岐、リトライポリシー。
4. 可視化 UI: 実行状態、各ノードの I/O、標準出力/エラー、実行時間/コストを表示。
5. 履歴/再現: 実行メタデータ保存、再実行（seed/環境含む）、結果差分。
6. テレメトリ: OpenTelemetry でトレース/メトリクス収集、簡易ダッシュボード。

## 追加/将来機能（候補）
- 評価データセット管理と自動ベンチマーク、ルーティング推奨（Best-of-N/ルール/バンディット）。
- 共有/エクスポート（ノートブック/レポート）。
- プロジェクト/権限/監査ログ。

## 非機能要件
- 性能: 10 並列以上のノード実行、UI レイテンシ < 200ms（表示更新）。
- 信頼性: ノード単位のリトライ、途中失敗時の部分結果保持、再開。
- セキュリティ: 最小権限 Sandbox、入力値サニタイズ、機密情報のマスキング。
- 可搬性/UI: ローカル開発（Mac/Linux）で動作し、UI はローカル Web。Windows は後日検討。

## 制約 / 前提
- 最初はローカル優先（SQLite/ファイルベース）。
- 外部 API キーはユーザー持ち。送信の可視化と同意フローを備える。
- OSS ライセンス互換性と再配布の留意。
 - データ保持: MVP は最小限（保持期間は要決定）。

## オープンクエスチョン（要ヒアリング）
- ローンチ目標時期/マイルストーンはいつか?
- データ保持期間/匿名化方針/監査要件の詳細は?
- Windows 対応の要否/時期は?

参照: `../01_requirements.md`
